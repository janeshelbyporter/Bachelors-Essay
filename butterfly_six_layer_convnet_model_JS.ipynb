{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import img_to_array, array_to_img, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# widths = []\n",
    "# heights = []\n",
    "\n",
    "# for dir in os.listdir(train_dir):\n",
    "#     dir_path = train_dir + \"/\" + dir\n",
    "#     if os.path.isdir(dir_path):\n",
    "#         for img in os.listdir(dir_path):\n",
    "#             img_path = os.path.join(train_dir + \"/\" + dir, img) # Making image file path\n",
    "#             im = Image.open(img_path)\n",
    "#             widths.append(im.size[0])\n",
    "#             heights.append(im.size[1])\n",
    "\n",
    "# AVG_HEIGHT = round(sum(heights)/len(heights))\n",
    "# AVG_WIDTH = round(sum(widths)/len(widths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVG_HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVG_WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default settings\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_dir = '/Users/Janeshelbyporter/Documents/OneDrive-CofC/BE/Image-Recognition-with-Deep-Learning-master/data/butterflies/train_01'\n",
    "train_dir2 = '/Users/Janeshelbyporter/Documents/OneDrive-CofC/BE/Image-Recognition-with-Deep-Learning-master/data/butterflies/train_02'\n",
    "validate_dir = '/Users/Janeshelbyporter/Documents/OneDrive-CofC/BE/Image-Recognition-with-Deep-Learning-master/data/butterflies/validate'\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n"
     ]
    }
   ],
   "source": [
    "# get number of images in training directory\n",
    "nb_train_samples = 0\n",
    "for r, dirs, files in os.walk(train_dir):\n",
    "    for dr in dirs:\n",
    "        nb_train_samples += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "print(nb_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "# get number of images in validation directory\n",
    "nb_validate_samples = 0\n",
    "for r, dirs, files in os.walk(validate_dir):\n",
    "    for dr in dirs:\n",
    "        nb_validate_samples += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "print(nb_validate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing for training\n",
    "train_datagen =  ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing for validation\n",
    "validate_datagen =  ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    fill_mode = 'nearest',\n",
    "    horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 592 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate and store training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# generate and store validation data\n",
    "validate_generator = validate_datagen.flow_from_directory(\n",
    "    validate_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def convnet_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                    input_shape=(3, img_width, img_height),\n",
    "                    activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                    activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** int(epoch / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate CNN model\n",
    "model_final = convnet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lr = 0.01\n",
    "sgd = SGD(learning_rate=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_final.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 00:59:58.418662: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 2.3058 - accuracy: 0.1115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 01:00:18.335219: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 22s 1s/step - loss: 2.3058 - accuracy: 0.1115 - val_loss: 2.2938 - val_accuracy: 0.1183 - lr: 0.0100\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 21s 1s/step - loss: 2.2892 - accuracy: 0.1318 - val_loss: 2.2826 - val_accuracy: 0.1775 - lr: 0.0100\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 21s 1s/step - loss: 2.2590 - accuracy: 0.1689 - val_loss: 2.2462 - val_accuracy: 0.2130 - lr: 0.0100\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 21s 1s/step - loss: 2.2032 - accuracy: 0.2027 - val_loss: 2.2892 - val_accuracy: 0.1243 - lr: 0.0100\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 21s 1s/step - loss: 2.1616 - accuracy: 0.1824 - val_loss: 2.1395 - val_accuracy: 0.2426 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2938acfa0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "# #     steps_per_epoch = nb_train_samples / batch_size,\n",
    "    validation_data = validate_generator,\n",
    "#     validation_steps = nb_validate_samples / batch_size,\n",
    "#     class_weight='auto',\n",
    "    callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
